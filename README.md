### Hi there ðŸ‘‹

My name is Niels, I'm 26 years old and I live in Belgium.

I'm currently working as an ML engineer @ [ðŸ¤—  HuggingFace](https://huggingface.co/), where I'm part of the Open-Source team.

I work on HuggingFace Transformers, a Python library implementing several state-of-the-art AI algorithms, all based on the original [Transformer](https://arxiv.org/abs/1706.03762) by Google.

I love making AI more accessible to anyone. So far, I've contributed the following algorithms to HuggingFace Transformers:
- [TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas), by Google AI
- [ViT](https://huggingface.co/docs/transformers/model_doc/vit), by Google AI
- [DEiT](https://huggingface.co/docs/transformers/model_doc/deit), by Facebook AI
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr), by Facebook AI
- [BEiT](https://huggingface.co/docs/transformers/model_doc/beit), by Microsoft Research
- [CANINE](https://huggingface.co/docs/transformers/model_doc/canine), by Google AI
- [LUKE](https://huggingface.co/docs/transformers/model_doc/luke), by Studio Ousia
- [LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2) and [LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm), by Microsoft Research
- [DINO](https://arxiv.org/abs/2104.14294), by Facebook AI
- [TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr), by Microsoft Research
- [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer), by NVIDIA
- [ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt), by OpenAI
- [Perceiver/Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver), by Deepmind
- [MAE](https://huggingface.co/docs/transformers/model_doc/vit_mae), by Facebook AI
- [ViLT](https://huggingface.co/docs/transformers/model_doc/vilt), by NAVER AI Lab
- [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), by Facebook AI
- [DiT](https://huggingface.co/docs/transformers/model_doc/dit), by Microsoft Research
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn), by KAIST (Korea Advanced Institute of Science and Technology) 
- [DPT](https://huggingface.co/docs/transformers/model_doc/dpt), by Intel Labs
- [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos), by School of EIC, Huazhong University of Science & Technology
- [TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex) by Microsoft Research
- [LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3), by Microsoft Research
- [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae), by Multimedia Computing Group, Nanjing University
- [Donut](https://huggingface.co/docs/transformers/model_doc/donut), by NAVER AI Lab
- [X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip), by Microsoft Research
- [Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr), by SenseTime Research
- [MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm), by Microsoft Research
- [LiLT](https://huggingface.co/docs/transformers/model_doc/lilt), South China University of Technology
- [Table Transformer](https://huggingface.co/docs/transformers/model_doc/table_transformer), by Microsoft Research
- [CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg), by University of GÃ¶ttingen
- [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/ast), by MIT Computer Science and Artificial Intelligence Laboratory, Cambridge
- [BiT](https://huggingface.co/docs/transformers/model_doc/bit), by Google AI
- [ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit), by Google AI
- [Swin2SR](https://huggingface.co/docs/transformers/main/model_doc/swin2sr), by CAIDAS, University of WÃ¼rzburg
- [GIT](https://huggingface.co/docs/transformers/main/model_doc/git), by Microsoft Research
- [UPerNet](https://huggingface.co/docs/transformers/main/model_doc/upernet), by Peking University
- [BLIP-2](https://huggingface.co/docs/transformers/main/model_doc/blip-2), by Salesforce

Besides that, I help others add models to the library, including:
- [Swin Transformer](https://arxiv.org/abs/2103.14030), by Microsoft Research
- [mLUKE](https://arxiv.org/abs/2110.08151), by Studio Ousia
- [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902), by University of Wisconsin-Madison
- [YOSO](https://arxiv.org/abs/2111.09714), by University of Wisconsin-Madison
- [PoolFormer](https://arxiv.org/abs/2111.11418), by Sea AI Lab
- [CvT](https://arxiv.org/abs/2103.15808), by Microsoft Research
- [GroupViT](https://arxiv.org/abs/2202.11094), by NVIDIA

I'm mostly working with PyTorch. For IDEs, I work with Visual Studio Code and Google Colab. It's all you need.

I learned everything about deep learning through self-study, mainly thanks to Andrej Karpathy's [cs231n course](http://cs231n.stanford.edu/) at Stanford University (lectures are free on Youtube and assigment solutions can be found on Github).
